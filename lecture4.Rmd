---
title: "Data science and analysis in Neuroscience"
author: "Kevin Allen"
date: "December 3, 2019"
output:
  ioslides_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(knitr)
```

## Today's plan

1. Combining information from several data frames
2. Common statistical procedures with R
  + Correlation 
  + t-test
  + ANOVA
  + Common assumptions and common errors


## Combining information from several data frames

Load the tmaze data set for a few more exercises.

```{r load}
myFile="~/repo/dataNeuroCourse/dataSets/tmaze.csv" 
df<-read_csv(myFile)
df<-mutate(df, correct = sample != choice)
```


## Combining information from several data frames

In most project, you need to work with several tables. 

Relations are defined between a pair of tables.

dplyr has several functions to do this.


## Combining information from several data frames

Let's assign a genotype to each mouse in a new data frame.

```{r relational1}
dfGeno<-tibble(mouse=c("Mn4656","Mn848","Mn4672","Mn4673",
                           "Mn7712","Mn7735","Mn829"),
           genotype=c("wt","wt","wt","wt",
                      "ko","ko","ko"))
dfGeno
```

***

How is `df` related to `dfGeno`?

```{r relational2}
colnames(df)
colnames(dfGeno)
colnames(dfGeno)[colnames(dfGeno) %in% colnames(df)]
```
`mouse` is a **key**, a variable that connect a pair of tables.

***

* A **primary key** uniquely identifies an observation in its table.

```{r relational3}
dfGeno %>% 
  count(mouse)

```

***

* A **foreign key** uniquely identifies an observation in *another* table.

```{r relational4}
df %>% 
  count(mouse)
```

## Mutating joins

* It first matches observation by their keys.
* Then copies across variables from one table to the other.

```{r relational5}
df_join <- df %>% 
  left_join(dfGeno,by="mouse") # match with mouse
df_join
```
We now have an additional variable (genotype) in df.

For more information: [Relational data and dplyr](https://r4ds.had.co.nz/relational-data.html)


## Common statistics with R

1. Linear correlation 
2. t-test
3. Wilcoxon tests
3. ANOVAs
4. Common errors in statistics

## General tip for statistics

Always plot the data. Don't do blind statistical testing. 

```{r, fig.width = 4, fig.height=3}
ggplot(data=mpg)+
  geom_point(mapping = aes(x=cty,y=displ),position="jitter")
```

[Garbage in, barbage out](https://en.wikipedia.org/wiki/Garbage_in,_garbage_out#:~:text=In%20computer%20science%2C%20garbage%20in,out%20(RIRO)%20is%20used)

## Input data

Functions calculating statistics often require **numeric vectors** as input. 

If you have a data frame, you may need to extract a single column.

```{r, results = 'hide'}
# classic R
mpg$displ
# or tidyverse style
mpg %>% pull(displ)
```

## Linear correlation (Pearson) 

Look for a linear relationships between two variables ($x$ and $y$).

Correlation coefficient ($r$).

$$ r_{xy} = \frac{\sum_{i=1}^{n} (x_i-\bar{x})(y_i-\bar{y})} {\sqrt{ \sum_{i=1}^{n} (x_i-\bar{x})^2  \sum_{i=1}^{n} (y_i-\bar{y})^2    }}  $$

## Degrees of freedom

The number of independent values that can vary in an analysis without breaking any constraints.

For a linear correlation: $df = n-2$

## Linear correlation

p value: From r to t, from t to p.

$t = \frac{r}{\sqrt{1-r^2}}\sqrt{n-2}$

```{r,fig.width = 4, fig.height=3.5}
t_val = seq(-5,5,0.1)
plot(t_val,dt(t_val,df=10),type='l',ylab="Probability",xlab="t")
```

## Example 1

First have a look at the input data

```{r example_correlation, fig.width=3,fig.height=3}
ggplot(data=attitude)+
  geom_point(mapping = aes(x=rating,y=complaints))
```

***

Then perform the calculations

```{r cor_example}
cor.test(attitude$rating,attitude$complaints)
```

## Example 2

Using the `mpg` data frame, test whether there is a linear relationship between the engine displacement and the city miles per gallon.

Tip: use `?mpg` if you want a reminder of what is in this data frame.

## Example 2: plot

```{r, fig.width = 4, fig.height=3}
ggplot(data=mpg)+
  geom_point(mapping = aes(x=cty,y=displ),position="jitter")
```

## Example 2: statistics

```{r}
cor.test(mpg$displ,mpg$cty)
```
Negative correlation coefficients are for negative slopes.

## Main assumptions of the linear correlation test

* The relationship is linear
* The data (x and y) are normally distributed (see shapiro.test()).
* The data points are independent of each other (not from same subject).

The p value has no real meaning if the assumptions of the test are violated.

Alternative: Spearman or Kendall rank correlation (method = c("pearson", "kendall", "spearman"))


## Warning






## Reading for this week

[Ten common statistical mistakes (eLife, 2019)](https://elifesciences.org/articles/48175?gclid=Cj0KCQiAwf39BRCCARIsALXWETyV3Dhu7jGevlenU1oh2zzLo812jOvAeparT0UYPEY9hPOFI34ECUQaAkBEEALw_wcB)
