---
title: "Data science and analysis in Neuroscience"
author: "Kevin Allen"
date: "December 3, 2020"
output:
  ioslides_presentation: default
---

```{r setup, include=FALSE}
library(tidyverse)
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(global.par = TRUE)
set.seed(114)
```

## Today's plan

1. Combining information from several data frames
2. Reshaping data frames
3. Common statistical procedures with R
  + Linear correlation 
  + t-test
  + Common errors in statistics

## Combining information from several data frames

Load the tmaze data set for a few more exercises.

```{r load, message = FALSE}
myFile="~/repo/dataNeuroCourse/dataSets/tmaze.csv" 
df<-read_csv(myFile)
df<-mutate(df, correct = sample != choice)
print(df,n=5)
```



## Combining information from several data frames

In most projects, you need to work with several tables. 

Relations are defined between a pair of tables.

`dplyr` has several **join** functions to do this.


## Combining information from several data frames

```{r relational1}
dfGeno <- tibble(mouse=c("Mn4656","Mn848","Mn4672","Mn4673",
                           "Mn7712","Mn7735","Mn829"),
           genotype=c("wt","wt","wt","wt",
                      "ko","ko","ko"))
dfGeno
```

***

How is `df` related to `dfGeno`?

```{r relational2}
colnames(df)
colnames(dfGeno)
colnames(dfGeno)[colnames(dfGeno) %in% colnames(df)]
```
`mouse` is a **key**, a variable that connect a pair of tables.

***

* A **primary key** uniquely identifies an observation in its table.

```{r relational3}
dfGeno %>% 
  count(mouse)

```

***

* A **foreign key** uniquely identifies an observation in *another* table.

```{r relational4}
df %>% 
  count(mouse)
```

## Mutating joins

* It first matches observations by their keys.
* Then copies across variables from one table to the other.

```{r relational5}
df_join <- df %>% 
  left_join(dfGeno,by="mouse") # match with mouse
print(df_join,n=6)
```

## Want to know more

[Chapter 10, Relational data and dplyr](https://r4ds.had.co.nz/relational-data.html)

## Tidy dataframes

The golden rules of **tidy** data frames

1. Each variable must have its own column.
2. Each observation must have its own row.

Advantages

1. You only need to learn how to process one type of data frame
2. dplyr and ggplot are designed to work with tidy data.

## Tidy dataframes

You will eventually encounter data sets that are not tidy.

What is wrong with this data frame?
```{r tb}
df <- tibble(country = c("Afghanistan","Brazil","China"),
       "1999" = c(745, 37737, 80488),
       "2000" = c(2666, 80488,213766))
df
```

## Tidy dataframes


```{r tb2, echo=FALSE}
df
```

Some of the columns are not variables but values of a variable.

Year is a variable. 1999 and 2000 are values of the variable year.

## Reshaping data frames into tidy data frames

We need to gather these columns into a new pair of variables.
```{r gathering}
df %>% gather('1999','2000', key = 'year', value = 'cases')
```

Now you can use `dplyr::group_by` and `dplyr::summarize` for example

## Reshaping data frames into tidy data frames

[Chapter 9, Tidy Data with tidyr](https://r4ds.had.co.nz/tidy-data.html)

There is a example with a data set from the World Health Organization.


## Common statistics with R

1. Linear correlation 
2. t-test
3. Common errors in statistics

## Why do we need statistical tests

```{r echo=F}
set.seed(113)
```

When can we say that our results are not due to chance?

```{r}
n=10
x <- rnorm (n = n, mean = 0, sd = 2)
y <- rnorm (n = n, mean = 0, sd = 2)
head(x,n=5)
head(y,n=5)
print(paste("Means :", round(mean(x),3),", ", round(mean(y),3)))
```


## General tip for statistics

Always plot the data. Don't do blind statistical testing. 

```{r set, echo = FALSE}
par(bty = 'n', mar=c(4,4,1,0.1), mgp=c(2,1,0)) 
```

```{r, fig.width = 4, fig.height=3}
ggplot(data=mpg)+
  geom_point(mapping = aes(x=cty,y=displ))
```

Prevent the ["garbage in, garbage out"](https://en.wikipedia.org/wiki/Garbage_in,_garbage_out#:~:text=In%20computer%20science%2C%20garbage%20in,out%20(RIRO)%20is%20used) effect.

## Input data

Functions calculating statistics most often require **numeric vectors** as input. 

If you have a data frame, you may need to extract a single column.

```{r, results = 'hide'}
# classic R
mpg$displ
# or tidyverse style
mpg %>% pull(displ)
```


## Linear correlation (Pearson) 

Look for a linear relationships between two variables ($x$ and $y$).

The typical plot for correlation analysis is a scatter plot.

```{r, fig.width = 3, fig.height=2.5}
ggplot(data=attitude)+
  geom_point(mapping = aes(x=learning,y=complaints)) +
  xlab("Opportunity to learn") +
  ylab("Handing of complaints")
```

## Linear correlation

1. We will do an example **manually**
2. Then an example with the cor.test() function.
3. Then you will have a go.


## Linear correlation

Let's generate data set to work with.

We will do an example *manually* to remove most of the magic from it.

```{r lincor,fig.width = 4, fig.height=3.5}
n = 40
# x = random numbers from a gaussian distribution
x <- rnorm (n = n, mean = 0, sd = 1)
y <- 2 * x + 50 + rnorm( n = n, mean = 0, sd = 2.0)
df <- data.frame(x = x, y = y)
head(df,n=5)
```
It is often useful when learning to work with simulated data.

## Linear correlation

Correlation coefficient ($r$).

$$ r_{xy} = \frac{\sum_{i=1}^{n} (x_i-\bar{x})(y_i-\bar{y})} {\sqrt{ \sum_{i=1}^{n} (x_i-\bar{x})^2  \sum_{i=1}^{n} (y_i-\bar{y})^2    }}  $$

$r$ varies from -1 to 1.

Negative $r$ values represent negative slopes.

1 or -1 is a perfect linear relationship.

Values near 0 mean very poor correlations.

## Linear correlation


```{r example,fig.width = 3, fig.height=2.2}
r_value = sum( (x-mean(x)) * (y - mean(y))) / 
  sqrt( sum((x-mean(x))^2) * sum((y-mean(y))^2))
print(paste("My r value:", round(r_value,2)))
df %>% ggplot()+
  geom_point(mapping = aes(x=x,y=y))+
  geom_smooth(mapping = aes(x=x,y=y),method = "lm",alpha=0.2)
```

## Linear correlation, degrees of freedom

The number of independent values that can vary in an analysis without breaking any constraints.

For a linear correlation: $df = n-2$

Nowadays, I use the value of $df$ to make sure the computer is doing what I think it is doing.

## Linear correlation, p value

We now need to decide whether there is a significant relationship between `x` and `y`. 

The $p$ value tells you the probability of obtaining a specific $r$ value (`r round(r_value,2)`) by chance. 

If it is less than 5%, we claim that there is a significant relationship between x and y.

Notice that you will get this wrong once every 20 attempts.

## Linear correlation, p value

$t = \frac{r}{\sqrt{1-r^2}}\sqrt{n-2}$

```{r}
t_value = r_value * sqrt(n-2) / sqrt(1-r_value^2)
print(paste("t:",round(t_value,4), 
            ", p:", round(1-pt(t_value,df = n-2),4)))
```

```{r,fig.width = 4, fig.height=3.0, echo=FALSE}
t <- seq(-8,8,0.1)
p <- dt(t,df=n-2)
df <- data.frame(t = t, prob = p)
df %>% ggplot +
  geom_line(mapping = aes(x=t,y=prob))
```

## Example 1

We are done with manually calculating r, t and p. Let's see how you normally do this in R.

We will use a data set (attitude) available within R.

```{r example_correlation, fig.width=3,fig.height=2.5}
ggplot(attitude) +
  geom_point(mapping = aes(x=learning,y=complaints)) +
  xlab("Opportunity to learn") +
  ylab("Handing of complaints")
```

## Example 1

All the calculation is done by `cor.test()`.

```{r cor_example}
cor.test(attitude$learning,attitude$complaints)
```
If p < 0.05, there is a linear relationship between the two variables.

## Example 1

You can also save the results for later use.

```{r cor_exp2}
res <- cor.test(attitude$learning,attitude$complaints)
print(paste("r:", res$estimate, res$estimate^2))
print(paste("t:",res$statistic))
print(paste("p:",res$p.value))
```

## Example 2

Using the `mpg` data frame, test whether there is a linear relationship between the engine displacement and the city miles per gallon.

Tip: use `?mpg` if you want a reminder of what is in this data frame.

You have 4 minutes to complete this task.

## Example 2: plot

```{r, fig.width = 4, fig.height=3}
ggplot(data=mpg)+
  geom_point(mapping = aes(x=cty,y=displ),position="jitter")
```

## Example 2: statistics

```{r}
cor.test(mpg$displ,mpg$cty)
```
Negative correlation coefficients are for negative slopes.

## Assumptions for linear correlations

* The relationship is linear
* The data (x and y) are normally distributed (see shapiro.test()).
* The data points are independent of each other (not from same subject).

The p value has no real meaning if the assumptions of the test are violated.

Alternative: **Spearman** or **Kendall** rank correlation (method = c("pearson", "kendall", "spearman"))


## Linear correlations: possible pitfalls
```{r seed2, echo=FALSE}
set.seed(111)
```

Things can go wrong with non-linear relationships

```{r nonlinear,fig.width = 4, fig.height=3}
n=50
x <- rnorm (n = n, mean = 0, sd = 1) 
y <- x^2 + rnorm(n = n, mean =0, sd = 0.2)
res <- cor.test(x,y)
plot(x,y,main=paste("r:",round(res$estimate,3),
                    ", p:",round(res$p.value,3)))
```


## Linear correlations: possible pitfalls

Random data
```{r change, echo = FALSE}
n=10
```
```{r outl,fig.height = 3, fig.width= 4}
x <- rnorm (n = n, mean = 0, sd = 1) 
y <- rnorm (n = n, mean = 0, sd = 1)
res <- cor.test(x,y)
plot(x,y,main=paste("r:",round(res$estimate,3),
                    ", p:",round(res$p.value,3)))
```

## Linear correlations: possible pitfalls

Random data and **one outlier**
```{r outl2,fig.height = 3, fig.width= 4}
x <-c(x, 4)
y <-c(y, 4)
res <- cor.test(x,y)
plot(x,y,main=paste("r:",round(res$estimate,3),
                    ", p:",round(res$p.value,3)))
```
 
## t-test

Test if there is a difference between

* the **mean** of one group to a known value (one-sample t test)
* the **means** of **2** groups (two-sample t test)
  * 2 independent groups
  * 2 dependent groups

Fun fact: Developed by William Sealy Gosset who worked as Head Brewer at Guinness in Dublin. He published under the pseudonym of "Student".

## One-sample t test

We want to test whether some values are significantly different from a value (e.g., 0).

More precisely, we want to know whether our data are sampled from a normal distribution with a specific mean.

Assumes that the data are normally distributed.

Degrees of freedom, $df = N-1$


## One-sample t test

$$ t = \frac{\bar{X} - \mu} {s/\sqrt{N}} $$
where $\bar{X}$ is the mean of your sample, $\mu$ is the mean of the population, $s$ is the standard deviation of your sample. 

With $t$ and your degrees of freedom, you can find the probability (p value) using the t distribution.

```{r,fig.width = 3, fig.height=2.5, echo=FALSE}
t <- seq(-8,8,0.1)
p <- dt(t,df=n-2)
df <- data.frame(t = t, prob = p)
df %>% ggplot +
  geom_line(mapping = aes(x=t,y=prob))
```




## One-sample t test example

We want to know whether the IQ of a group of undergraduate students is different than that of the general population (100).
We obtain scores from 12 students and perform a t-test.


## One-sample t test example

Don't forget to look at your data

```{r ttest_example,fig.height = 3, fig.width= 4}
# data entries
gs <- data.frame(IQ = c(103,123,95,132,113,102,98,97,110,102,112,98)) 
ggplot(data = gs) + 
  geom_histogram(mapping = aes(x=IQ),binwidth = 2)
```


## One-sample t test example

Test for normality with the Shapiro-Wilk test.

If p < 0.05, then your distribution is not normal.

```{r test_normality}
shapiro.test(gs$IQ)
```

## One-sample t test example

Use the `t.test()` function to perform the t test.

```{r ttest_example2}
t.test(x = gs$IQ,
       alternative = "two.sided",
       mu = 100)
```


## Two-sample t test

Comparing the means of **2** groups.

* Two independent groups (different cells or subjects)
* Two dependent groups (same cells or subjects tested twice)


## t test for independent samples

We measure the firing rate of the two types of neurons (pyramidal cells and interneurons) and want to know if their firing rates differs.


```{r twosamples}
neurons <- tibble(neuronType = c(rep("pyr",8),rep("int",8)),
  rate = c(2.5,1.4,3.2,2.5,3.4,2.5,4.6,3.0,5.2,1.7,6.2,2.7,7.8,4.9,5.6,3.2))
head(neurons,n=5)
```

## t test for independent samples

We should always plot our data. For t-tests, a boxplot is a good idea.

```{r boxp, fig.height=3,fig.width=4}
neurons %>% ggplot()+
  geom_boxplot(mapping = aes(x = neuronType,y = rate))
```


## t test for independent samples

Use 2 numerical vectors as inputs.

```{r testindepended}
pyr <- neurons %>% filter(neuronType=="pyr") %>% select(rate) %>% pull()
int <- neurons %>% filter(neuronType=="int") %>% select(rate) %>% pull()
t.test(x = pyr, y = int,
       alternative = "two.sided", paired = FALSE)
```
## t test for independent samples

Use the arguments `data` and `formula`. Simpler with tidy data sets.

```{r testindepended2}
t.test(formula = rate~neuronType,
       alternative = "two.sided", paired = FALSE, data = neurons)
```
## t test for independent samples

Assumptions

*  The two distributions have the same variance
*  The two groups are independent
*  Normality


`t.test()` by default will take into consideration the unequal sample size, like in our example.

Alternative for data that are not normally distributed: Wilcoxon Rank Sum Tests `wilcox.test()`.


## t test for dependent samples

We measure the firing rate of the **same** neurons in two conditions (t0 and t1) and want to know if the firing rate changed.

```{r twosamples3}
neurons <- data.frame(t0 = c(2.5,1.4,3.2,2.5,3.4,2.5,4.6,3.0),
           t1 = c(2.7,1.7,3.2,2.7,3.8,2.9,4.6,3.2))
neurons
```

## Anything wrong with this data frame?

```{r echo = F}
neurons
```

## Anything wrong with this data frame?

```{r echo = F}
neurons
```

It has several observations per row. Harder to use ggplot.

## Reshape for ggplot

Here we reshape the data set.

```{r pivot}
neurons_long <- neurons %>%
  gather(key="condition", value = "rate", starts_with("t"))
neurons_long
```

## t test for dependent samples

Now it is easier to use ggplot.

```{r neurons_long, fig.height=2.5, fig.width = 3}
ggplot(data=neurons_long)+
  geom_boxplot(mapping = aes(x = condition,y=rate))
```


## t test for dependent samples

For paired t test, you need to vectors of equal length
```{r dependentt}
t.test(x=neurons$t0, y=neurons$t1,
       alternative = "two.sided", paired = TRUE)
```
Significant results despite a very small effect (0.2125)

## t test for dependent samples

The test works on the difference between t0 and t1.

```{r paireddiff,fig.width=3,fig.height=2.5}
df <- data.frame(pairedDiff = neurons$t0-neurons$t1)
df %>% ggplot()+
  geom_histogram(aes(x=pairedDiff),binwidth=0.05)
```

t test for dependent samples can detect smaller difference.

## Reading for this week

[Ten common statistical mistakes (eLife, 2019)](https://elifesciences.org/articles/48175?gclid=Cj0KCQiAwf39BRCCARIsALXWETyV3Dhu7jGevlenU1oh2zzLo812jOvAeparT0UYPEY9hPOFI34ECUQaAkBEEALw_wcB)

This article covers the most common statistical mistakes made by scientists. They are very common. 

Learn to detect them and eliminate them from your work.
